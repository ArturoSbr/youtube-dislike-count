{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a7606f",
   "metadata": {},
   "source": [
    "# Scrape YouTube data\n",
    "This notebook scrapes the general details of videos uploaded by multiple news channels between 2021-11-05 and 2021-11-15 as well as each video's details and comment sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4aed9",
   "metadata": {},
   "source": [
    "## Environment\n",
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1073101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import langid\n",
    "from youtube import channel, video\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf9d0c",
   "metadata": {},
   "source": [
    "Get API key from environment variables"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d18369c7",
   "metadata": {},
   "source": [
    "load_dotenv()\n",
    "key = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad8aac4",
   "metadata": {},
   "source": [
    "## Import list of News channels\n",
    "\n",
    "This data set is a handpicked list of news channels that:\n",
    "1. Are relevant (top 100 subscribed or viewed channels)\n",
    "1. Post political content in English\n",
    "1. Have open comment sections"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb5c0487",
   "metadata": {},
   "source": [
    "df = pd.read_csv('../../dat/top_news_channels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c94a7f",
   "metadata": {},
   "source": [
    "## 1. Build `channels` table\n",
    "\n",
    "NOTE @ 2022-03-14: This section has already been executed. Since the requests are expensive, it's best to just load the result."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5d53b6f",
   "metadata": {},
   "source": [
    "# Initialize list\n",
    "data = []\n",
    "\n",
    "# Get channels' details\n",
    "for channelId in df['channelId'].values:\n",
    "    ch = channel(id=channelId, key=key)\n",
    "    data.append([channelId] + ch.get_info())\n",
    "\n",
    "\n",
    "# Write table\n",
    "df1 = pd.DataFrame(data=data, columns=['channelId','name','joined','views'])\n",
    "df1.to_csv('../../dat/channels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load result\n",
    "df1 = pd.read_csv('../../dat/channels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf666a",
   "metadata": {},
   "source": [
    "## 2. Build `channelVideos` table\n",
    "\n",
    "NOTE @ 2022-03-15: This section has already been executed. Since the requests are expensive, it's best to just load the result.\n",
    "\n",
    "### 2.1. Pre-treatment videos\n",
    "Videos uploaded on or before 2021-11-09"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef2e98ab",
   "metadata": {},
   "source": [
    "# Initialize list\n",
    "vids_pre = []\n",
    "\n",
    "# Pre-treatment videos\n",
    "for channelId in df1['channelId'].values:\n",
    "    ch = channel(id=channelId, key=key)\n",
    "    try:\n",
    "        videoIds = ch.get_videos(category=25, date0='2021-11-05T00:00:00', date1='2021-11-09T23:59:59')\n",
    "        for videoId in videoIds:\n",
    "            vids_pre.append([channelId, videoId, 0])\n",
    "    except:\n",
    "        print('Crashed on channel', channelId)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0338d1e0",
   "metadata": {},
   "source": [
    "### 2.2. Post-treatment videos\n",
    "Videos uploaded on or after 2021-11-11 (skip November 10th because the policy was gradually rolled out)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29acd3ad",
   "metadata": {},
   "source": [
    "# Initialize list\n",
    "vids_post = []\n",
    "\n",
    "# Post-treatment videos\n",
    "for channelId in df1['channelId'].values:\n",
    "    ch = channel(id=channelId, key=key)\n",
    "    try:\n",
    "        videoIds = ch.get_videos(category=25, date0='2021-11-11T00:00:00', date1='2021-11-15T23:59:59')\n",
    "        for videoId in videoIds:\n",
    "            vids_post.append([channelId, videoId, 1])\n",
    "    except:\n",
    "        print('Crashed on channel', channelId)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f5d270",
   "metadata": {},
   "source": [
    "API quota ran out on `channelId = UCt-WqkTyKK1_70U4bb4k4lQ`."
   ]
  },
  {
   "cell_type": "raw",
   "id": "67c1886c",
   "metadata": {},
   "source": [
    "# Next index before crash\n",
    "idx = df1[df1['channelId'].eq('UCt-WqkTyKK1_70U4bb4k4lQ')].index[0]\n",
    "\n",
    "# Loop resumes\n",
    "for channelId in df1.iloc[idx:, 0]:\n",
    "    ch = channel(id=channelId, key=key)\n",
    "    try:\n",
    "        videoIds = ch.get_videos(category=25, date0='2021-11-11T00:00:00', date1='2021-11-15T23:59:59')\n",
    "        for videoId in videoIds:\n",
    "                vids_post.append([channelId, videoId, 1])\n",
    "    except:\n",
    "        print('Crashed on channel', channelId)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a1baf",
   "metadata": {},
   "source": [
    "Export table"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6c5738b",
   "metadata": {},
   "source": [
    "df2 = pd.DataFrame(data=vids_pre + vids_post, columns=['channelId','videoId','treat'])\n",
    "df2.to_csv('../../dat/videos.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b76a50a",
   "metadata": {},
   "source": [
    "Create backup of `videos.csv` table before adding videos posted on fuzzy day"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc54e778",
   "metadata": {},
   "source": [
    "# Load result before adding 2021-11-10 videos\n",
    "df2 = pd.read_csv('../../dat/videos.csv')\n",
    "\n",
    "# Create backup\n",
    "df2.to_csv('../../dat/videosBackup.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9501e31b",
   "metadata": {},
   "source": [
    "NOTE @ 2022-08-24: This section has already been executed. Since the requests are expensive, it's best to just load the result.\n",
    "\n",
    "### 2.3. Fuzzy-day videos\n",
    "\n",
    "Videos uploaded on 1010-11-10"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a816970e",
   "metadata": {},
   "source": [
    "# Initialize list\n",
    "vids_on = []\n",
    "\n",
    "# Post-treatment videos\n",
    "for channelId in df1['channelId'].values:\n",
    "    ch = channel(id=channelId, key=key)\n",
    "    try:\n",
    "        videoIds = ch.get_videos(category=25, date0='2021-11-10T00:00:00', date1='2021-11-10T23:59:59')\n",
    "        for videoId in videoIds:\n",
    "            vids_on.append([channelId, videoId, np.nan])\n",
    "    except:\n",
    "        print('Crashed on channel', channelId)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d8bc28",
   "metadata": {},
   "source": [
    "Add videos posted on 2022-11-10 to `videos.csv` table"
   ]
  },
  {
   "cell_type": "raw",
   "id": "595a446f",
   "metadata": {},
   "source": [
    "# Read OG videos table\n",
    "df2 = pd.read_csv('../../dat/videosBackup.csv')\n",
    "\n",
    "# Fuzzy videos to pandas\n",
    "t = pd.DataFrame(data=vids_on, columns=list(df2.columns))\n",
    "\n",
    "# Append fuzzy videos\n",
    "df2 = pd.concat([df2, t], axis=0)\n",
    "\n",
    "# Replace table\n",
    "df2.to_csv('../../dat/videos.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd548f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load result\n",
    "df2 = pd.read_csv('../../dat/videos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78932f",
   "metadata": {},
   "source": [
    "## 3. Build `videoDetails` table\n",
    "\n",
    "Get the details of each video (title, description, duration, definition, etc.). These data will be used as controls.\n",
    "\n",
    "NOTE @ 2022-03-15: This section has already been executed. Since the requests are expensive, it's best to just load the result."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d70c9e3a",
   "metadata": {},
   "source": [
    "# Initialize list\n",
    "data = []\n",
    "\n",
    "# Loop videos\n",
    "for videoId in df2['videoId'].values:\n",
    "    try:\n",
    "        vid = video(id=videoId, key=key).get_details()\n",
    "        data.append([videoId] + list(vid.values()))\n",
    "    except:\n",
    "        print('Crashed on videoId', videoId)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572db078",
   "metadata": {},
   "source": [
    "Export table"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41f7f818",
   "metadata": {},
   "source": [
    "df3 = pd.DataFrame(data=data, columns=['videoId'] + list(vid.keys()))\n",
    "df3.to_csv('../../dat/videoDetails.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33234500",
   "metadata": {},
   "source": [
    "# Load result\n",
    "df3 = pd.read_csv('../../dat/videoDetails.csv')\n",
    "\n",
    "# Create backup\n",
    "df3 = df3.to_csv('../../dat/videoDetailsBackup.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac15a9",
   "metadata": {},
   "source": [
    "Mine details of fuzzy-day videos."
   ]
  },
  {
   "cell_type": "raw",
   "id": "16b16a39",
   "metadata": {},
   "source": [
    "# Initialize list\n",
    "data = []\n",
    "\n",
    "# Loop videos\n",
    "for videoId in df2.loc[df2['treat'].isna(), 'videoId'].values:\n",
    "    try:\n",
    "        vid = video(id=videoId, key=key).get_details()\n",
    "        data.append([videoId] + list(vid.values()))\n",
    "    except:\n",
    "        print('Crashed on videoId', videoId)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a325d26",
   "metadata": {},
   "source": [
    "Append new details to `videoDetails.csv` backup table"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab4595ae",
   "metadata": {},
   "source": [
    "# Load backup\n",
    "df3 = pd.read_csv('../../dat/videoDetailsBackup.csv')\n",
    "\n",
    "# New data to pandas\n",
    "t = pd.DataFrame(data=data, columns=list(df3.columns))\n",
    "\n",
    "# Append new data to old table\n",
    "df3 = pd.concat([df3, t], axis=0)\n",
    "\n",
    "# Overwrite OG\n",
    "df3.to_csv('../../dat/videoDetails.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fde294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df3 = pd.read_csv('../../dat/videoDetails.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a18de8",
   "metadata": {},
   "source": [
    "## 4. Build `videoComments` table\n",
    "\n",
    "### 4.1. Mine all comment sections"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32b9bf90",
   "metadata": {},
   "source": [
    "for videoId in df2['videoId'].values:\n",
    "    comments = video(id=videoId, key=key).get_comments()\n",
    "    if len(comments) > 0:\n",
    "        json.dump({videoId:comments}, open('../../dat/comments/' + videoId + '.json', 'w'))\n",
    "    else:\n",
    "        # Manual check\n",
    "        print(videoId)\n",
    "        #break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f2af3",
   "metadata": {},
   "source": [
    "- Quota Exceeded on `videoId = '_laKJi8Xwh8'`\n",
    "- Quota Exceeded on `videoId = 'OzRR6ROQ-mA'`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d81ca82a",
   "metadata": {},
   "source": [
    "# Resume from crash\n",
    "idx = df2.loc[df2['videoId'].eq('OzRR6ROQ-mA')].index[0] + 1\n",
    "\n",
    "for videoId in df2.iloc[idx:, 1].values:\n",
    "    comments = video(id=videoId, key=key).get_comments()\n",
    "    if len(comments) > 0:\n",
    "        json.dump({videoId:comments}, open('../../dat/comments/' + videoId + '.json', 'w'))\n",
    "    else:\n",
    "        # Manual check\n",
    "        print(videoId)\n",
    "        #break"
   ]
  },
  {
   "cell_type": "raw",
   "id": "391578bc",
   "metadata": {},
   "source": [
    "# Manual error count\n",
    "commsDis = [\n",
    "    'AoLgaqj9Q7s','wZDvWH45BUQ','UAtSE-ytvk4','cdDUfKHerGI','e44OO1DqImk','oKD0YkxdfYA','gIbiEzje_mo',\n",
    "    'heIGpJU3WyI','iP-H28T1XGg','xrGhSRCnXbk','4jblalZNrNs','dds1f6xkGfo','zjDVNb5UShM','6U5jf2RJC28',\n",
    "    'wTsrq94A6n8','7vEWTyp-AhQ','OodybxmgZjM','r9O5KJYFCIM','fkr5raCVVBg','E0dnSJNJ1Jo','7Y3uS_WCyvM',\n",
    "    '2kpoj0-F1Jk','Wd8X4FMoAPI','hr-fc1DcQyM','xr07bsxCLas','tt33Q_qgGcg'\n",
    "]\n",
    "errors = [\n",
    "    'vQNl8PpcSHw','bu7wwMIrxak','kujtF7tZ1Zk','cdDUfKHerGI','sKzgl4KTbhk','jzyCqHYIkvE','N2A4eI3xXWQ',\n",
    "    'crBfKSXy-gU','FBoZK9ng-II'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa4182",
   "metadata": {},
   "source": [
    "Mine videos uploaded on 2021-11-10"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b87e8b24",
   "metadata": {},
   "source": [
    "# Start mining\n",
    "for videoId in df2.loc[df2['treat'].isna(), 'videoId'].values:\n",
    "    comments = video(id=videoId, key=key).get_comments()\n",
    "    if len(comments) > 0:\n",
    "        json.dump({videoId:comments}, open('../../dat/comments/' + videoId + '.json', 'w'))\n",
    "    else:\n",
    "        # Manual check\n",
    "        print(videoId)\n",
    "        #break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8909440e",
   "metadata": {},
   "source": [
    "### 4.2. Count and classify relevant comments\n",
    "Each video's negative comment ratio is defined as\n",
    "$$ncr_i(h) = \\frac\n",
    "    {\\text{Negative comments }| \\text{ Post time} \\leq \\text{Upload time} + h}\n",
    "    {\\text{Comments } | \\text{ Post time} \\leq \\text{Upload time} + h}$$\n",
    "for $i \\in \\{1, ..., n\\}$ and $h \\in \\{12, 24, ..., 72\\}$\n",
    "\n",
    "---\n",
    "\n",
    "Note: Not a single comment must overlap with the 10th!!!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "259ec7c5",
   "metadata": {},
   "source": [
    "# Initialize sentiment analyzer\n",
    "clf = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Files available on HDD\n",
    "files = [file.replace('.json','') for file in os.listdir('../../dat/comments/')]\n",
    "\n",
    "# Turn publishedAt to pandas.DateTime\n",
    "df3['dateUpload'] = pd.to_datetime(df3['publishedAt'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# Initialize dictionary to store info\n",
    "d = {}\n",
    "\n",
    "# Iterate over all time windows\n",
    "for h in [i * 12 for i in range(1, 7)]:\n",
    "    # Initialize list\n",
    "    d['post' + str(h)] = []\n",
    "    # Set the maximum possible timestamp for comments in each videoId\n",
    "    df3['dateMax'] = df3['dateUpload'] + pd.DateOffset(hours=h)\n",
    "    # Create mask to include videos on HDD and not on banned window\n",
    "    m = df3['videoId'].isin(files) & ((df3['dateMax'] < '2021-11-10') | (df3['dateUpload'] >= '2021-11-11'))\n",
    "    # Iterate over (videoId, maxDate)\n",
    "    for videoId, dateMax in df3.loc[m, ['videoId','dateMax']].itertuples(index=False):\n",
    "        # maxDate to YYYY-MM-DDTHH:MM:DD:SSZ\n",
    "        dateMax = str(dateMax).replace(' ','T') + 'Z'\n",
    "        # Read videoId's comment section\n",
    "        comments = json.load(open('../../dat/comments/' + videoId + '.json'))\n",
    "        # Initialize counters\n",
    "        commentsNum, commentsNeg1, commentsNeg2 = 0, 0, 0\n",
    "        # Iterate over comments in coment section\n",
    "        for comment in comments[videoId]:\n",
    "            # Check if within time window\n",
    "            if comment['publishedAt'] <= dateMax:\n",
    "                # Count comment\n",
    "                commentsNum += 1\n",
    "                # Score comment\n",
    "                score = clf.polarity_scores(comment['textOriginal'])\n",
    "                # Count if overall sentiment is negative\n",
    "                if score['compound'] < 0:\n",
    "                    commentsNeg1 += 1\n",
    "                # Count if comment has *some* negative sentiment\n",
    "                if score['neg'] > 0:\n",
    "                    commentsNeg2 += 1\n",
    "        # Update list\n",
    "        d['post' + str(h)].append([videoId, commentsNum, commentsNeg1, commentsNeg2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53934597",
   "metadata": {},
   "source": [
    "Export `videoFlags` table"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03e7cabf",
   "metadata": {},
   "source": [
    "# Column names\n",
    "cols = ['CommentsNum','CommentsNeg1','CommentsNeg2']\n",
    "\n",
    "# First table (post12)\n",
    "t = pd.DataFrame(d['post12'], columns=['videoId'] + ['post12' + col for col in cols])\n",
    "\n",
    "# Merge remaining tables\n",
    "for key in list(d.keys())[1:]:\n",
    "    temp = pd.DataFrame(d[key], columns=['videoId'] + [key + col for col in cols])\n",
    "    t = t.merge(temp, on='videoId', how='outer')\n",
    "    del temp\n",
    "    \n",
    "# Export data\n",
    "t.to_csv('../../dat/videoFlags.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e55246",
   "metadata": {},
   "source": [
    "### 4.3. Count and classify comments (including `2022-11-10`)\n",
    "Count types of comments **without language check**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70da5ff5",
   "metadata": {},
   "source": [
    "# Initialize tone classifier\n",
    "clf = SentimentIntensityAnalyzer()\n",
    "\n",
    "# All possible time windows\n",
    "windows = [i * 12 for i in range(1, 7)]\n",
    "\n",
    "# Files to parse\n",
    "files = sorted([file.replace('.json', '') for file in os.listdir('../../dat/comments/')])\n",
    "\n",
    "# Turn publishedAt to pandas.DateTime\n",
    "df3['dateUpload'] = pd.to_datetime(df3['publishedAt'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# Initialize dictionary to store videoId - nComms - NCR - sNCR - ...\n",
    "d = {}\n",
    "\n",
    "# Iterate over all time windows\n",
    "for h in windows:\n",
    "    \n",
    "    # Initialize list\n",
    "    d['post' + str(h)] = []\n",
    "    \n",
    "    # Set the maximum possible timestamp for comments in each videoId\n",
    "    df3['dateMax'] = df3['dateUpload'] + pd.DateOffset(hours=h)\n",
    "    \n",
    "    # Create mask to include videos on HDD\n",
    "    m = df3['videoId'].isin(files)\n",
    "    \n",
    "    # Iterate over (videoId, maxDate)\n",
    "    for videoId, dateMax in df3.loc[m, ['videoId','dateMax']].itertuples(index=False):\n",
    "        \n",
    "        # maxDate to YYYY-MM-DDTHH:MM:DD:SSZ\n",
    "        dateMax = str(dateMax).replace(' ','T') + 'Z'\n",
    "        \n",
    "        # Read videoId's comment section\n",
    "        comments = json.load(open(f'../../dat/comments/{videoId}.json'))\n",
    "        \n",
    "        # Initialize counters\n",
    "        commentsNum, commentsNeg1, commentsNeg2, commentsPos1, commentsPos2 = 0, 0, 0, 0, 0\n",
    "\n",
    "        # Iterate over comments in coment section\n",
    "        for comment in comments[videoId]:\n",
    "\n",
    "            # Check if within time window\n",
    "            if comment['publishedAt'] <= dateMax:\n",
    "                \n",
    "                # Count comment\n",
    "                commentsNum += 1\n",
    "                \n",
    "                # Score comment\n",
    "                score = clf.polarity_scores(comment['textOriginal'])\n",
    "\n",
    "                # Neg1: Count if overall sentiment is negative\n",
    "                if score['compound'] < 0:\n",
    "                    commentsNeg1 += 1\n",
    "                \n",
    "                # Neg2: Count if comment has *some* negative sentiment\n",
    "                if score['neg'] > 0:\n",
    "                    commentsNeg2 += 1\n",
    "                \n",
    "                # Pos1: Count if comment is positive\n",
    "                if score['compound'] > 0:\n",
    "                    commentsPos1 += 1\n",
    "                \n",
    "                # Pos2: Count if comment has *some* positive sentiment\n",
    "                if score['pos'] > 0:\n",
    "                    commentsPos2 += 1\n",
    "\n",
    "        # Update list\n",
    "        d['post' + str(h)].append([videoId, commentsNum, commentsNeg1, commentsNeg2, commentsPos1, commentsPos2])\n",
    "\n",
    "# Column names\n",
    "cols = ['CommentsNum','CommentsNeg1','CommentsNeg2','CommentsPos1','CommentsPos2']\n",
    "\n",
    "# First table (post12)\n",
    "t = pd.DataFrame(d['post12'], columns=['videoId'] + ['post12' + col for col in cols])\n",
    "\n",
    "# Merge remaining tables\n",
    "for key in list(d.keys())[1:]:\n",
    "    temp = pd.DataFrame(d[key], columns=['videoId'] + [key + col for col in cols])\n",
    "    t = t.merge(temp, on='videoId', how='outer')\n",
    "del temp\n",
    "    \n",
    "# Export data\n",
    "t.to_csv('../../dat/videoFlagsFuzzy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e55246",
   "metadata": {},
   "source": [
    "Count types of comments **with language check**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70da5ff5",
   "metadata": {},
   "source": [
    "# Initialize tone classifier\n",
    "clf = SentimentIntensityAnalyzer()\n",
    "\n",
    "# All possible time windows\n",
    "windows = [i * 12 for i in range(1, 7)]\n",
    "\n",
    "# JSON files to parse\n",
    "files = [file.replace('.json', '') for file in os.listdir('../../dat/comments/') if '.json' in file]\n",
    "\n",
    "# Turn publishedAt to pandas.DateTime\n",
    "df3['dateUpload'] = pd.to_datetime(df3['publishedAt'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# Initialize dictionary to store videoId - nComms - NCR - sNCR - ...\n",
    "d = {}\n",
    "\n",
    "# Create mask to include videos on HDD (used in line 29)\n",
    "m = df3['videoId'].isin(files)\n",
    "\n",
    "# Iterate over all time windows\n",
    "for h in windows:\n",
    "    \n",
    "    # Initialize list\n",
    "    d['post' + str(h)] = []\n",
    "    \n",
    "    # Set the maximum possible timestamp for comments in each videoId\n",
    "    df3['dateMax'] = df3['dateUpload'] + pd.DateOffset(hours=h)\n",
    "    \n",
    "    # Iterate over (videoId, maxDate)\n",
    "    for videoId, dateMax in df3.loc[m, ['videoId','dateMax']].itertuples(index=False):\n",
    "        \n",
    "        # maxDate to YYYY-MM-DDTHH:MM:DD:SSZ\n",
    "        dateMax = str(dateMax).replace(' ','T') + 'Z'\n",
    "        \n",
    "        # Read videoId's comment section\n",
    "        comments = json.load(open(f'../../dat/comments/{videoId}.json'))\n",
    "        \n",
    "        # Initialize counters\n",
    "        commentsNum, commentsNeg1, commentsNeg2, commentsPos1, commentsPos2 = 0, 0, 0, 0, 0\n",
    "\n",
    "        # Iterate over comments in coment section\n",
    "        for comment in comments[videoId]:\n",
    "\n",
    "            # Check if within time window\n",
    "            if comment['publishedAt'] <= dateMax:\n",
    "\n",
    "                # English comment flag\n",
    "                eng = langid.classify(comment['textOriginal'])[0] == 'en'\n",
    "\n",
    "                # Only count if English\n",
    "                if eng:\n",
    "                \n",
    "                    # Count comment\n",
    "                    commentsNum += 1\n",
    "                    \n",
    "                    # Score comment\n",
    "                    score = clf.polarity_scores(comment['textOriginal'])\n",
    "\n",
    "                    # Neg1: Count if overall sentiment is negative\n",
    "                    if score['compound'] < 0:\n",
    "                        commentsNeg1 += 1\n",
    "                    \n",
    "                    # Neg2: Count if comment has *some* negative sentiment\n",
    "                    if score['neg'] > 0:\n",
    "                        commentsNeg2 += 1\n",
    "                    \n",
    "                    # Pos1: Count if comment is positive\n",
    "                    if score['compound'] > 0:\n",
    "                        commentsPos1 += 1\n",
    "                    \n",
    "                    # Pos2: Count if comment has *some* positive sentiment\n",
    "                    if score['pos'] > 0:\n",
    "                        commentsPos2 += 1\n",
    "\n",
    "        # Update list\n",
    "        d['post' + str(h)].append([videoId, commentsNum, commentsNeg1, commentsNeg2, commentsPos1, commentsPos2])\n",
    "\n",
    "# Column names\n",
    "cols = ['CommentsNum','CommentsNeg1','CommentsNeg2','CommentsPos1','CommentsPos2']\n",
    "\n",
    "# First table (post12)\n",
    "t = pd.DataFrame(d['post12'], columns=['videoId'] + ['post12' + col for col in cols])\n",
    "\n",
    "# Merge remaining tables\n",
    "for key in list(d.keys())[1:]:\n",
    "    temp = pd.DataFrame(d[key], columns=['videoId'] + [key + col for col in cols])\n",
    "    t = t.merge(temp, on='videoId', how='outer')\n",
    "del temp\n",
    "    \n",
    "# Export data\n",
    "t.to_csv('../../dat/videoFlagsFuzzyLangid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e9f3b",
   "metadata": {},
   "source": [
    "Sample 1000 comments and score them (this is used for comparing the classifications made by VADR and my own labels)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02289855",
   "metadata": {},
   "source": [
    "# Place all comments in a list\n",
    "t = []\n",
    "for vid in os.listdir('../../dat/comments'):\n",
    "    if vid[-5:] == '.json':\n",
    "        f = json.load(open(f'../../dat/comments/{vid}', 'r')).get(vid[:-5])\n",
    "        for comment in f:\n",
    "            t.append(comment.get('textOriginal'))\n",
    "\n",
    "# All comments to pandas df\n",
    "t = pd.DataFrame({'comment':t})\n",
    "\n",
    "# Sample 2000 comments and only keep English ones\n",
    "t = t.sample(n=2000, random_state=42)\n",
    "t['eng'] = t['comment'].apply(lambda x: langid.classify(x)[0] == 'en')\n",
    "\n",
    "# Sample 1000 comments from survivors\n",
    "t = t.sample(n=1000, random_state=42)\n",
    "\n",
    "# Score each comment\n",
    "vadr = SentimentIntensityAnalyzer()\n",
    "vadr.polarity_scores('pepe')\n",
    "s = []\n",
    "for comment in t['comment']:\n",
    "    s.append(list(vadr.polarity_scores(comment).values()))\n",
    "s = pd.DataFrame(data=s, columns=['neg','neu','pos','com'])\n",
    "t = pd.concat([t.reset_index(drop=True), s], axis=1)\n",
    "\n",
    "# Classify comments according to criteria\n",
    "t = t.assign(\n",
    "    ncr1Vadr = np.where(t['com'] < 0, 1, 0),\n",
    "    ncr2Vadr = np.where(t['neg'] > 0, 1, 0),\n",
    "    ncr1Hand = np.nan,\n",
    "    ncr2Hand = np.nan\n",
    ")\n",
    "\n",
    "# Export\n",
    "t.to_csv('../../dat/comments_sample.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "95c0f0991ffcdb038fcf97c3f0d49464981cd58006e8af0cf678ddb2337346a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
