{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a7606f",
   "metadata": {},
   "source": [
    "# Scrape YouTube data\n",
    "This notebook scrapes the general details of videos uploaded by multiple news channels between 2021-11-05 and 2021-11-15 as well as each video's details and comment sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4aed9",
   "metadata": {},
   "source": [
    "## Environment\n",
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1073101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from youtube import channel, video\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf9d0c",
   "metadata": {},
   "source": [
    "Get API key from environment variables"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d18369c7",
   "metadata": {},
   "source": [
    "load_dotenv()\n",
    "key = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad8aac4",
   "metadata": {},
   "source": [
    "## Import list of News channels\n",
    "\n",
    "This data set is a handpicked list of news channels that:\n",
    "1. Are relevant (top 100 subscribed or viewed channels)\n",
    "1. Post political content in English\n",
    "1. Have open comment sections"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb5c0487",
   "metadata": {},
   "source": [
    "df = pd.read_csv('../../dat/top_news_channels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c94a7f",
   "metadata": {},
   "source": [
    "## 1. Build `channels` table\n",
    "\n",
    "NOTE @ 2021-03-14: This section has already been executed. Since the requests are expensive, it's best to just load the result."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5d53b6f",
   "metadata": {},
   "source": [
    "# Initialize list\n",
    "data = []\n",
    "\n",
    "# Get channels' details\n",
    "for channelId in df['channelId'].values:\n",
    "    ch = channel(id=channelId, key=key)\n",
    "    data.append([channelId] + ch.get_info())\n",
    "\n",
    "\n",
    "# Write table\n",
    "df1 = pd.DataFrame(data=data, columns=['channelId','name','joined','views'])\n",
    "df1.to_csv('../../dat/channels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load result\n",
    "df1 = pd.read_csv('../../dat/channels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf666a",
   "metadata": {},
   "source": [
    "## 2. Build `channelVideos` table\n",
    "\n",
    "NOTE @ 2021-03-15: This section has already been executed. Since the requests are expensive, it's best to just load the result.\n",
    "\n",
    "### 2.1. Pre-treatment videos\n",
    "Videos uploaded on or before 2021-11-09"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef2e98ab",
   "metadata": {},
   "source": [
    "# Initialize list\n",
    "vids_pre = []\n",
    "\n",
    "# Pre-treatment videos\n",
    "for channelId in df1['channelId'].values:\n",
    "    ch = channel(id=channelId, key=key)\n",
    "    try:\n",
    "        videoIds = ch.get_videos(category=25, date0='2021-11-05T00:00:00', date1='2021-11-09T23:59:59')\n",
    "        for videoId in videoIds:\n",
    "            vids_pre.append([channelId, videoId, 0])\n",
    "    except:\n",
    "        print('Crashed on channel', channelId)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0338d1e0",
   "metadata": {},
   "source": [
    "### 2.2. Post-treatment videos\n",
    "Videos uploaded on or after 2021-11-11 (skip November 10th because the policy was gradually rolled out)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29acd3ad",
   "metadata": {},
   "source": [
    "# Initialize list\n",
    "vids_post = []\n",
    "\n",
    "# Post-treatment videos\n",
    "for channelId in df1['channelId'].values:\n",
    "    ch = channel(id=channelId, key=key)\n",
    "    try:\n",
    "        videoIds = ch.get_videos(category=25, date0='2021-11-11T00:00:00', date1='2021-11-15T23:59:59')\n",
    "        for videoId in videoIds:\n",
    "            vids_post.append([channelId, videoId, 1])\n",
    "    except:\n",
    "        print('Crashed on channel', channelId)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f5d270",
   "metadata": {},
   "source": [
    "API quota ran out on `channelId = UCt-WqkTyKK1_70U4bb4k4lQ`."
   ]
  },
  {
   "cell_type": "raw",
   "id": "67c1886c",
   "metadata": {},
   "source": [
    "# Next index before crash\n",
    "idx = df1[df1['channelId'].eq('UCt-WqkTyKK1_70U4bb4k4lQ')].index[0]\n",
    "\n",
    "# Loop resumes\n",
    "for channelId in df1.iloc[idx:, 0]:\n",
    "    ch = channel(id=channelId, key=key)\n",
    "    try:\n",
    "        videoIds = ch.get_videos(category=25, date0='2021-11-11T00:00:00', date1='2021-11-15T23:59:59')\n",
    "        for videoId in videoIds:\n",
    "                vids_post.append([channelId, videoId, 1])\n",
    "    except:\n",
    "        print('Crashed on channel', channelId)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a1baf",
   "metadata": {},
   "source": [
    "Export table"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6c5738b",
   "metadata": {},
   "source": [
    "df2 = pd.DataFrame(data=vids_pre + vids_post, columns=['channelId','videoId','treat'])\n",
    "df2.to_csv('../../dat/videos.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc54e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load result\n",
    "df2 = pd.read_csv('../../dat/videos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78932f",
   "metadata": {},
   "source": [
    "## 3. Build `videoDetails` table\n",
    "\n",
    "Get the details of each video (title, description, duration, definition, etc.). These data will be used as controls.\n",
    "\n",
    "NOTE @ 2021-03-15: This section has already been executed. Since the requests are expensive, it's best to just load the result."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d70c9e3a",
   "metadata": {},
   "source": [
    "# Initialize list\n",
    "data = []\n",
    "\n",
    "# Loop videos\n",
    "for videoId in df2['videoId'].values:\n",
    "    try:\n",
    "        vid = video(id=videoId, key=key).get_details()\n",
    "        data.append([videoId] + list(vid.values()))\n",
    "    except:\n",
    "        print('Crashed on videoId', videoId)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572db078",
   "metadata": {},
   "source": [
    "Export table"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41f7f818",
   "metadata": {},
   "source": [
    "df3 = pd.DataFrame(data=data, columns=['videoId'] + list(vid.keys()))\n",
    "df3.to_csv('../../dat/videoDetails.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33234500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load result\n",
    "df3 = pd.read_csv('../../dat/videoDetails.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a18de8",
   "metadata": {},
   "source": [
    "## 4. Build `videoComments` table\n",
    "\n",
    "### 4.1. Mine all comment sections"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32b9bf90",
   "metadata": {},
   "source": [
    "for videoId in df2['videoId'].values:\n",
    "    comments = video(id=videoId, key=key).get_comments()\n",
    "    if len(comments) > 0:\n",
    "        json.dump({videoId:comments}, open('../../dat/comments/' + videoId + '.json', 'w'))\n",
    "    else:\n",
    "        # Manual check\n",
    "        print(videoId)\n",
    "        #break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f2af3",
   "metadata": {},
   "source": [
    "- Quota Exceeded on `videoId = '_laKJi8Xwh8'`\n",
    "- Quota Exceeded on `videoId = 'OzRR6ROQ-mA'`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d81ca82a",
   "metadata": {},
   "source": [
    "# Resume from crash\n",
    "idx = df2.loc[df2['videoId'].eq('OzRR6ROQ-mA')].index[0] + 1\n",
    "\n",
    "for videoId in df2.iloc[idx:, 1].values:\n",
    "    comments = video(id=videoId, key=key).get_comments()\n",
    "    if len(comments) > 0:\n",
    "        json.dump({videoId:comments}, open('../../dat/comments/' + videoId + '.json', 'w'))\n",
    "    else:\n",
    "        # Manual check\n",
    "        print(videoId)\n",
    "        #break"
   ]
  },
  {
   "cell_type": "raw",
   "id": "391578bc",
   "metadata": {},
   "source": [
    "# Manual error count\n",
    "comdis = ['AoLgaqj9Q7s','wZDvWH45BUQ','UAtSE-ytvk4','cdDUfKHerGI','e44OO1DqImk','oKD0YkxdfYA','gIbiEzje_mo',\n",
    "          'heIGpJU3WyI','iP-H28T1XGg','xrGhSRCnXbk','4jblalZNrNs','dds1f6xkGfo','zjDVNb5UShM','6U5jf2RJC28',\n",
    "          'wTsrq94A6n8','7vEWTyp-AhQ','OodybxmgZjM','r9O5KJYFCIM','fkr5raCVVBg','E0dnSJNJ1Jo','7Y3uS_WCyvM',\n",
    "          '2kpoj0-F1Jk','Wd8X4FMoAPI','hr-fc1DcQyM','xr07bsxCLas','tt33Q_qgGcg']\n",
    "errors = ['vQNl8PpcSHw','bu7wwMIrxak','kujtF7tZ1Zk','cdDUfKHerGI','sKzgl4KTbhk','jzyCqHYIkvE','N2A4eI3xXWQ',\n",
    "          'crBfKSXy-gU','FBoZK9ng-II']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8909440e",
   "metadata": {},
   "source": [
    "### 4.2. Count and classify relevant comments\n",
    "Function that classifies text as negative or non-negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf(text=None):\n",
    "    r = SentimentIntensityAnalyzer().polarity_scores(text)\n",
    "    if r['neg'] == max(list(r.values())[:3]):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ac772d",
   "metadata": {},
   "source": [
    "For every video, from the comments posted at most one day after its release, count the total number of comments as well as the total number of negative comments.\n",
    "\n",
    "To do:\n",
    "- Repeat procedure to get `yt` for $t \\in \\{1,2,3,4,5\\}$\n",
    "- Pass on videos as a function of $t$"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1714da55",
   "metadata": {},
   "source": [
    "# Initialize list [[videoId, nComments, nNegativeComments]]\n",
    "commentSections = []\n",
    "\n",
    "# Loop all .json files\n",
    "for fileName in os.listdir('../../dat/comments/'):\n",
    "    # Check if file is json (to avoid hidden OS files)\n",
    "    if fileName.split('.')[1] == 'json':\n",
    "        # Get videoId from file name\n",
    "        videoId = fileName[:-5]\n",
    "        # Add t days to upload date\n",
    "        maxDate = df3.loc[df3['videoId'].eq(videoId), 'publishedAt'].item()\n",
    "        maxDate = maxDate[:5] + str(int(maxDate[5:7]) + 1) + maxDate[7:]\n",
    "        # Get all comments from file\n",
    "        comments = list(json.load(open('../../dat/comments/' + fileName, 'r'))[videoId])\n",
    "        # Initialize counters\n",
    "        nComments = 0\n",
    "        nNegativeComments = 0\n",
    "        # Loop all comments\n",
    "        for comment in comments:\n",
    "            if comment['publishedAt'] <= maxDate:\n",
    "                nComments += 1\n",
    "                nNegativeComments += clf(text=comment['textOriginal'])\n",
    "            else:\n",
    "                pass\n",
    "        commentSections.append([videoId, nComments, nNegativeComments])\n",
    "    # Avoid .DS_Store crashes\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Export\n",
    "df4 = pd.DataFrame(data=commentSections, columns=['videoId','nComments','nNegativeComments'])\n",
    "df4.to_csv('../../dat/comments_classified_t1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8751438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read t1\n",
    "df4 = pd.read_csv('../../dat/comments_classified_t1.csv')\n",
    "# Add publishedAt\n",
    "df = pd.merge(df3, df4, on='videoId', how='inner')\n",
    "# Remove November 9\n",
    "df = df[df['publishedAt'].str[8:10] != '09']\n",
    "# Add treatment column\n",
    "df = df.merge(df2[['videoId','treat']])\n",
    "# Declare target\n",
    "df['ncr1'] = df['nNegativeComments'].div(df['nComments'])\n",
    "# Days until treatment\n",
    "df['dUntil'] = 10 - df['publishedAt'].str[8:10].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bcd70d",
   "metadata": {},
   "source": [
    "- The policy took place on November 10th\n",
    "- $t = 1 \\implies$ videos posted on 11-09 and 11-10 have comments posted on the 10th\n",
    "    - Discard 11-09\n",
    "    - Discard 11-10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
