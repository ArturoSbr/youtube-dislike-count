{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19693b80",
   "metadata": {
    "id": "19693b80"
   },
   "source": [
    "# Analysis\n",
    "\n",
    "This notebook creates features from raw tables, runs multiple regressions and visualizes the results.\n",
    "\n",
    "## 1. Set environment\n",
    "\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17591853",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17591853",
    "outputId": "e919d802-8ac8-465d-9f60-e555f75b91b7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import statsmodels.api as sm\n",
    "from isodate import parse_duration\n",
    "from scipy.stats import ttest_ind\n",
    "from stargazer.stargazer import Stargazer\n",
    "\n",
    "# Constantly changing\n",
    "from disco import cv_bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e10a6b",
   "metadata": {},
   "source": [
    "The following cell parses json files. Avoid running it again."
   ]
  },
  {
   "cell_type": "raw",
   "id": "aaacf8f1",
   "metadata": {},
   "source": [
    "# Imports for this cell\n",
    "import os\n",
    "import json\n",
    "\n",
    "# List of all files\n",
    "files = [file for file in os.listdir('../../dat/comments/') if '.json' in file]\n",
    "\n",
    "# Count all comments scraped\n",
    "allComments = 0\n",
    "for file in files:\n",
    "    cs = json.load(open(f'../../dat/comments/{file}'))\n",
    "    allComments += len(\n",
    "        cs.get(\n",
    "            file.replace('.json', '')\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Results\n",
    "print(f'Number of videos with at least one comment in first 12 hours:{len(files)}') # 1,846\n",
    "print(f'Number of comments scraped: {allComments}') # 1,197,454"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee4cd6e",
   "metadata": {},
   "source": [
    "Counts:\n",
    "- All videos\n",
    "    - 1,928\n",
    "- All videos with at least one comment in first 12 hours\n",
    "    - 1,846\n",
    "- All videos with at least one comment in English in first 12 hours\n",
    "    - 1,814\n",
    "- All videos with at least one comment in first 12 hours and excluding fuzzy window\n",
    "    - 1516\n",
    "- All videos with at least one comment in English in first 12 hours excluding fuzzy window\n",
    "    - 1504\n",
    "- All comments\n",
    "    - 1,197,454"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b141c",
   "metadata": {
    "id": "b78b141c"
   },
   "source": [
    "Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a85ed",
   "metadata": {
    "id": "178a85ed"
   },
   "outputs": [],
   "source": [
    "# Video details table\n",
    "d1 = pd.read_csv('../../dat/videoDetails.csv')\n",
    "\n",
    "# Classified comments\n",
    "# d2 = pd.read_csv('../../dat/videoFlags.csv') # Deprecated (no langid filter & no Nov-10 data)\n",
    "# d2 = pd.read_csv('../../dat/videoFlagsFuzzy.csv') # Deprecated (no langid filter)\n",
    "d2 = pd.read_csv('../../dat/videoFlagsFuzzyLangid.csv') # Current (has langid filter and Nov-10 data)\n",
    "\n",
    "# Create dataframe for analysis\n",
    "df = pd.merge(d1, d2, on='videoId', how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1abc6e",
   "metadata": {},
   "source": [
    "Create masks for each time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b88dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert publishedAt to timestamp\n",
    "df['publishedAt'] = pd.to_datetime(df['publishedAt'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# Start of policy rollout\n",
    "start = pd.Timestamp('2021-11-10 00:00:00')\n",
    "\n",
    "# Time windows\n",
    "hours = np.arange(12, 72+1, 12)         # [12, 24, ..., 72]\n",
    "windows = [f'Post{h}' for h in hours]   # [Post12, Post24, ..., Post72]\n",
    "\n",
    "# Init dict in which to store masks\n",
    "donuts = {}\n",
    "\n",
    "# Create masks for each time window\n",
    "for h in hours:\n",
    "\n",
    "    # Set max time of upload before treatment\n",
    "    lim = start - pd.Timedelta(hours=h)\n",
    "\n",
    "    # Pre-treatment or post-treatment mask (h: mask)\n",
    "    donuts[h] = df['publishedAt'].le(lim) | df['publishedAt'].ge('2021-11-11')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c563fb",
   "metadata": {
    "id": "75c563fb"
   },
   "source": [
    "## 2. Feature engineering\n",
    "Turn `definition` to dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a69d2",
   "metadata": {
    "id": "b74a69d2"
   },
   "outputs": [],
   "source": [
    "df['definition'] = df['definition'].replace({'sd':'0','hd':'1'}).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67536050",
   "metadata": {
    "id": "67536050"
   },
   "source": [
    "Create means from counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b37125f",
   "metadata": {
    "id": "2b37125f"
   },
   "outputs": [],
   "source": [
    "# Create per-video means\n",
    "for h in hours:\n",
    "\n",
    "    # NCRs\n",
    "    df[f'ncr1Post{h}'] = df[f'post{h}CommentsNeg1'] / df[f'post{h}CommentsNum'].replace(0, 1)\n",
    "    df[f'ncr2Post{h}'] = df[f'post{h}CommentsNeg2'] / df[f'post{h}CommentsNum'].replace(0, 1)\n",
    "    \n",
    "    # PCRs\n",
    "    df[f'pcr1Post{h}'] = df[f'post{h}CommentsPos1'] / df[f'post{h}CommentsNum'].replace(0, 1)\n",
    "    df[f'pcr2Post{h}'] = df[f'post{h}CommentsPos2'] / df[f'post{h}CommentsNum'].replace(0, 1)\n",
    "    \n",
    "    # Relative ratios (constraint: at least one of each)\n",
    "    df[f'rel1Post{h}'] = np.where(\n",
    "        df[f'post{h}CommentsNeg1'].gt(0) & df[f'post{h}CommentsPos1'].gt(0) & df[f'post{h}CommentsNum'].ge(5),\n",
    "        df[f'post{h}CommentsNeg1'] / df[f'post{h}CommentsPos1'],\n",
    "        np.nan\n",
    "    )\n",
    "    df[f'rel2Post{h}'] = np.where(\n",
    "        df[f'post{h}CommentsNeg2'].gt(0) & df[f'post{h}CommentsPos2'].gt(0) & df[f'post{h}CommentsNum'].ge(5),\n",
    "        df[f'post{h}CommentsNeg2'] / df[f'post{h}CommentsPos2'],\n",
    "        np.nan\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a5c574",
   "metadata": {
    "id": "53a5c574"
   },
   "source": [
    "Analyze most important words in video titles to create dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3156b940",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "3156b940",
    "outputId": "e2ad3e11-0d95-4528-d39b-5d0771ecc114"
   },
   "outputs": [],
   "source": [
    "# All titles to single text\n",
    "text = ' '.join(df['title'].str.title().tolist())\n",
    "\n",
    "# Remove annoying strings\n",
    "for string in [\"'s\",\".\",\"-\"]:\n",
    "    text = text.replace(string, '')\n",
    "\n",
    "# All words to uppercase\n",
    "text = text.upper()\n",
    "# Stopwords\n",
    "stopwords = set(list(STOPWORDS) + ['SAY','SAYS','S'])\n",
    "\n",
    "# Plot wordcloud\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    max_words=25,\n",
    "    stopwords=stopwords,\n",
    "    max_font_size=40, \n",
    "    scale=3,\n",
    "    random_state=42\n",
    ").generate(text)\n",
    "\n",
    "# Show wordcloud\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "plt.axis('off')\n",
    "plt.imshow(wordcloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65939766",
   "metadata": {},
   "source": [
    "Create dummy variables by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f56b12",
   "metadata": {
    "id": "10f56b12"
   },
   "outputs": [],
   "source": [
    "# Title to lowercase\n",
    "df['title'] = df['title'].str.lower()\n",
    "\n",
    "# Dictionary of keywords\n",
    "topics = {\n",
    "    'biden':'biden',\n",
    "    'trump':'trump',\n",
    "    'president':'biden|trump',\n",
    "    'climate':'cop26|cop 26|climate',\n",
    "    'economy':'inflation|infrastructure|bill|economy',\n",
    "    'covid':'covid|covid19|covid-19|virus',\n",
    "    'violence':'kill|murder|assassins| die|dead|shoot|shot'\n",
    "}\n",
    "\n",
    "# Create Indicator variables\n",
    "for topic in topics.keys():\n",
    "    df[topic] = np.where(df['title'].str.contains(topics[topic]), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b285d",
   "metadata": {
    "id": "2e6b285d"
   },
   "source": [
    "Video title sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83289431",
   "metadata": {
    "id": "83289431"
   },
   "outputs": [],
   "source": [
    "clf = SentimentIntensityAnalyzer()\n",
    "df['toneCom'] = df['title'].apply(lambda x: clf.polarity_scores(x)['compound'])\n",
    "df['tonePos'] = df['title'].apply(lambda x: clf.polarity_scores(x)['pos'])\n",
    "df['toneNeg'] = df['title'].apply(lambda x: clf.polarity_scores(x)['neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba47247d",
   "metadata": {
    "id": "ba47247d"
   },
   "source": [
    "Translate `duration` to seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471d484e",
   "metadata": {
    "id": "471d484e"
   },
   "outputs": [],
   "source": [
    "# YT-duration format to seconds\n",
    "df['seconds'] = df['duration'].apply(lambda x: int(parse_duration(x).total_seconds()))\n",
    "\n",
    "# log(seconds)\n",
    "df['logSeconds'] = np.log(df['seconds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7712005",
   "metadata": {
    "id": "a7712005"
   },
   "source": [
    "Sort data by upload date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ad18a4",
   "metadata": {
    "id": "f4ad18a4"
   },
   "outputs": [],
   "source": [
    "df = df.sort_values('publishedAt', ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79daa2da",
   "metadata": {
    "id": "79daa2da"
   },
   "source": [
    "Treatment indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a317e68b",
   "metadata": {
    "id": "a317e68b"
   },
   "outputs": [],
   "source": [
    "df['treat'] = (df['publishedAt'] >= '2021-11-10').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71750b13",
   "metadata": {
    "id": "71750b13"
   },
   "source": [
    "Declare running variable $R_i$ and interaction term $R_i \\times T_i$\n",
    "- Before: Seconds until treatment (control was positive, treatment was negative)\n",
    "- Update: Seconds since treatment (control is negative, treatment is positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c4df5",
   "metadata": {
    "id": "6f3c4df5"
   },
   "outputs": [],
   "source": [
    "# Running variable\n",
    "df['r'] = (df['publishedAt'] - pd.Timestamp('2021-11-10')).dt.total_seconds()\n",
    "\n",
    "# Interaction\n",
    "df['rTreat'] = df['r'].multiply(df['treat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b283eaad",
   "metadata": {
    "id": "b283eaad"
   },
   "source": [
    "## 3. Descriptive Statistics\n",
    "\n",
    "### Counts\n",
    "\n",
    "Number of available videos as a function of $h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9718a7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "a9718a7d",
    "outputId": "5c16e638-831e-45ac-af65-8a8a3c45e0b7"
   },
   "outputs": [],
   "source": [
    "# All post%HCommentsNum columns\n",
    "cols = [f'post{str(h)}CommentsNum' for h in hours]\n",
    "\n",
    "# Merge to get videoId & post{h}CommentsNum (full list of videos!)\n",
    "t = pd.merge(d1[['videoId','publishedAt']], d2[['videoId'] + cols], on='videoId', how='left')\n",
    "\n",
    "# publishedAt to Timestamp\n",
    "t['publishedAt'] = pd.to_datetime(t['publishedAt'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# Create pre & post groups\n",
    "t['treat'] = t['publishedAt'] >= '2021-11-10'\n",
    "\n",
    "# Mask each column to avoid overlap with 2021-11-10\n",
    "for h, col in zip(hours, cols):\n",
    "    t[col] = t['publishedAt'].le(pd.Timestamp('2021-11-10') - pd.Timedelta(hours=h)) | t['publishedAt'].ge('2021-11-11')\n",
    "\n",
    "# Group by treatment and get counts\n",
    "t = t.groupby('treat').agg(dict(zip(cols, ['sum']*7))).transpose()\n",
    "\n",
    "# Total number of videos column\n",
    "t['total'] = t.sum(axis=1)\n",
    "\n",
    "# Format\n",
    "t.index = ['h = ' + str(h) for h in windows]\n",
    "t\n",
    "\n",
    "# To latex\n",
    "# print(\n",
    "#    t.to_latex(\n",
    "#         caption='Number of available videos before and after November 10, 2021 for different values of $h$',\n",
    "#         label='tab_dat_nobs'\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df25858b",
   "metadata": {
    "id": "df25858b"
   },
   "source": [
    "### Balance tests\n",
    "\n",
    "Balance table using a linear stepwise design and excluding the donut hole for $h = 12$.\n",
    "\n",
    "$$X_i = \\gamma_0 + \\gamma_1 r_i + \\gamma_2 T_i + \\gamma_3 (r_i \\times T_i) + V_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82bd8c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "b82bd8c2",
    "outputId": "c6f5dd0b-864a-4ebf-8fbe-2449e36addbf"
   },
   "outputs": [],
   "source": [
    "# Mask to avoid overlap with November 10 (h = 12)\n",
    "mask = df['r'].le(-12*60**2) | df['r'].ge(24*60**2)\n",
    "print(f'{mask.sum()} videos used in balance test.')\n",
    "\n",
    "# Copy data\n",
    "d = df[mask].copy()\n",
    "\n",
    "# Create video length (minutes)\n",
    "d['durationMins'] = d['seconds'].div(60)\n",
    "\n",
    "# Order frequent-word variables by frequency\n",
    "X = list(topics.keys()) + ['definition','durationMins','tonePos','toneNeg','toneCom']\n",
    "\n",
    "# Regress each variable on r and treat\n",
    "data = []\n",
    "for x in X:\n",
    "    m = sm.OLS.from_formula(\n",
    "        data=d,\n",
    "        formula=f'{x} ~ r + treat + I(r*treat)',\n",
    "    ).fit(cov_type='HC0')\n",
    "    data.append((m.params['treat'], m.pvalues['treat']))\n",
    "\n",
    "# Summary table\n",
    "t = pd.DataFrame(data=data, index=X, columns=['Estimated Value','p-value'])\n",
    "t.index.rename('Covariate', inplace=True)\n",
    "t.reset_index(inplace=True)\n",
    "\n",
    "# Print exog that did not pass balance test\n",
    "print('Variables that did not pass: \\n', t.loc[t['p-value'].lt(0.1), 'Covariate'], sep='')\n",
    "\n",
    "# View\n",
    "t.round(3)\n",
    "\n",
    "# To latex\n",
    "# print(\n",
    "#     t.to_latex(\n",
    "#         caption='Regression discontinuities on observable characteristics',\n",
    "#         label='tab_dat_balance',\n",
    "#         float_format='%.3f',\n",
    "#         index=False\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d677e",
   "metadata": {
    "id": "4f5d677e"
   },
   "source": [
    "## 4. Regression Analysis\n",
    "\n",
    "Convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368226f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stars function\n",
    "def stars(pval):\n",
    "    if pval <= 0.01:\n",
    "        return '***'\n",
    "    elif pval <= 0.05:\n",
    "        return '**'\n",
    "    elif pval <= 0.1:\n",
    "        return '*'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Function to extract most relevant info from RDD\n",
    "def get_info(models, dep_vars, hours):\n",
    "\n",
    "    # Form dataframe\n",
    "    t =  pd.DataFrame(\n",
    "        {\n",
    "            # Targets\n",
    "            'y':np.repeat(dep_vars, len(hours)),\n",
    "            # Time windows\n",
    "            'h':list(hours)*len(dep_vars),\n",
    "            # Betas\n",
    "            'T':[models[i].params['treat'] for i in range(len(models))],\n",
    "            # Standard errors\n",
    "            'se':[models[i].bse['treat'] for i in range(len(models))],\n",
    "            # p-values\n",
    "            'pval':[models[i].pvalues['treat'] for i in range(len(models))],\n",
    "            # Significance\n",
    "            'signif':[stars(models[i].pvalues['treat']) for i in range(len(models))],\n",
    "            # Mean at r=0\n",
    "            'mean':[models[i].predict({'r':0, 'treat':0}).item() for i in range(len(models))],\n",
    "            # R2\n",
    "            'R2':[models[i].rsquared for i in range(len(models))],\n",
    "            # Number of observations\n",
    "            'nobs':[int(models[i].nobs) for i in range(len(models))]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Reindex table\n",
    "    return t.set_index(['y', 'h'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9769ad5",
   "metadata": {},
   "source": [
    "### 4.1. Fit models\n",
    "The following models consider a donut hole as a funtion of $h$.\n",
    "\n",
    "`d1:` $Y_i = \\beta_0 + \\beta_1 r_i + \\beta_2 T_i + \\beta_3 (T_i \\times r_i) + U_i$\n",
    "\n",
    "`d2:` $Y_i = \\beta_0 + \\beta_1 r_i + \\beta_2 r_i^2 + \\beta_3 T_i + \\beta_4 (T_i \\times r_i) + \\beta_5 (T_i \\times r_i)^2 + V_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b351ad",
   "metadata": {
    "id": "b2b351ad",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename post{H}CommentsNum to make it compatible\n",
    "for col in df.columns:\n",
    "    if 'CommentsNum' in col:\n",
    "        df.rename(\n",
    "            columns={col:f'commentsNum{col[:6].title()}'},\n",
    "            inplace=True\n",
    "        )\n",
    "\n",
    "# Minimum number of comments in each video\n",
    "min_comments = 0\n",
    "\n",
    "# Empty lists to store results in\n",
    "d1, d2 = [], []\n",
    "\n",
    "# Iterate over targets\n",
    "for target in ['ncr1', 'ncr2', 'pcr1', 'pcr2', 'rel1', 'rel2', 'commentsNum']:\n",
    "\n",
    "    # Iterate over Post%H windows\n",
    "    for h, post in zip(hours, windows):\n",
    "\n",
    "        # Masks for `rel1` & `rel2`\n",
    "        if 'rel' in target:\n",
    "            mask = donuts[h] & (df[f'{target}{post}'].notna())\n",
    "        \n",
    "        # Masks for `ncr`, `pcr` and `commentsNum`\n",
    "        else:\n",
    "            mask = donuts[h] & (df[f'commentsNum{post}'] > min_comments)\n",
    "\n",
    "        # Linear and quad formulas for post{h}CommentsNum\n",
    "        p1 = f'{target}{post} ~ treat + r + I(r*treat)'\n",
    "        p2 = f'{target}{post} ~ treat + r + I(r**2) + I(r*treat) + I((r*treat)**2)'\n",
    "\n",
    "        # Fit models\n",
    "        m1 = sm.OLS.from_formula(formula=p1, data=df[mask]).fit(cov_type='HC0')\n",
    "        m2 = sm.OLS.from_formula(formula=p2, data=df[mask]).fit(cov_type='HC0')\n",
    "\n",
    "        # Append to list\n",
    "        d1.append(m1)\n",
    "        d2.append(m2)\n",
    "\n",
    "# Create summary tables\n",
    "d1Res = get_info(d1, ['ncr1', 'ncr2', 'pcr1', 'pcr2', 'rel1', 'rel2', 'commentsNum'], hours)\n",
    "d2Res = get_info(d2, ['ncr1', 'ncr2', 'pcr1', 'pcr2', 'rel1', 'rel2', 'commentsNum'], hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbe456b",
   "metadata": {},
   "source": [
    "View results from linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b83524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results\n",
    "d1ResNCR1 = d1Res[d1Res.index.get_level_values(0) == 'ncr1'].reset_index().transpose()\n",
    "d1ResNCR2 = d1Res[d1Res.index.get_level_values(0) == 'ncr2'].reset_index().transpose()\n",
    "d1ResPCR1 = d1Res[d1Res.index.get_level_values(0) == 'pcr1'].reset_index().transpose()\n",
    "d1ResPCR2 = d1Res[d1Res.index.get_level_values(0) == 'pcr2'].reset_index().transpose()\n",
    "\n",
    "# Results to Latex\n",
    "# print(\n",
    "#     d1ResNCR1.to_latex(\n",
    "#         caption='Results for NCR',\n",
    "#         label='tab_res_d1ncr1',\n",
    "#         float_format='%.3f',\n",
    "#         index=True\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# View results\n",
    "d1Res.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb054ec",
   "metadata": {},
   "source": [
    "View results from quadratic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3960da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View\n",
    "d2Res.round(3)\n",
    "\n",
    "# To latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9926ce5e",
   "metadata": {},
   "source": [
    "### 4.2. Robustness checks\n",
    "\n",
    "#### 4.2.1. Fit linear models without donut hole\n",
    "Ignore donut holes and consider treatment at `2021-11-10 00:00:00`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ccb88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists to store results in\n",
    "r1 = []\n",
    "\n",
    "# Iterate over targets\n",
    "for target in ['ncr1', 'ncr2', 'pcr1', 'pcr2', 'rel1', 'rel2','commentsNum']:\n",
    "\n",
    "    # Iterate over Post%H windows\n",
    "    for h, post in zip(hours, windows):\n",
    "\n",
    "        # Mask commentsNumPost{h} > 0)\n",
    "        mask = df[f'commentsNum{post}'] > 0\n",
    "\n",
    "        # Formula for first-degree polynomial\n",
    "        p1 = f'{target}{post} ~ treat + r + I(r*treat)'\n",
    "\n",
    "        # Fit models\n",
    "        m1 = sm.OLS.from_formula(formula=p1, data=df[mask]).fit(cov_type='HC0')\n",
    "\n",
    "        # Append to list\n",
    "        r1.append(m1)\n",
    "\n",
    "# Get results\n",
    "r1Res = get_info(r1, ['ncr1', 'ncr2', 'pcr1', 'pcr2', 'rel1', 'rel2', 'commentsNum'], hours)\n",
    "\n",
    "# View results\n",
    "r1Res.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771f1d6",
   "metadata": {},
   "source": [
    "#### 4.2.2. Linear models with optimum bandwidth\n",
    "Find MSE-optimal bandwidth using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b087313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init list to store results from CVs in\n",
    "cvs = []\n",
    "\n",
    "# Iterate over time windows\n",
    "for hour in hours:\n",
    "\n",
    "    # Custom bandwidths for each time period (plus some pad)\n",
    "    pad = 1\n",
    "    bws = [(60**2)*h for h in np.arange(-144, 144+1) if abs(h) >= max(abs(hour), 24)+pad]\n",
    "\n",
    "    # Data that excludes donut hole\n",
    "    t = df[donuts[hour]].copy()\n",
    "\n",
    "    # CV process\n",
    "    cv = cv_bandwidth(\n",
    "        data=t,\n",
    "        dependent_variable=f'ncr1Post{hour}',\n",
    "        running_variable='r',\n",
    "        cutoff=0,\n",
    "        treated='above',\n",
    "        degree=1,\n",
    "        bandwidths=bws,\n",
    "        folds=5,\n",
    "        criteria='mse',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Sort by MSE DESC\n",
    "    cv.sort_values('cvScore', ascending=False, inplace=True)\n",
    "\n",
    "    # Convert bounds to hours\n",
    "    cv[['lowerBoundH', 'upperBoundH']] = cv[['lowerBound', 'upperBound']].div(60**2).astype(int)\n",
    "\n",
    "    # Add window column\n",
    "    cv['window'] = hour\n",
    "\n",
    "    # Append to list\n",
    "    cvs.append(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f888643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best bw for each window\n",
    "opt = pd.concat(\n",
    "    [cvs[i].head(1) for i in range(len(cvs))]\n",
    ")\n",
    "\n",
    "# Winners\n",
    "opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62379d87",
   "metadata": {},
   "source": [
    "Train model with optimal bandwidths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa08e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init list to store models in\n",
    "m_opt = []\n",
    "\n",
    "# Iterate over hours\n",
    "for i, hour in enumerate(hours):\n",
    "\n",
    "    # Optimum bounds\n",
    "    lb = opt.iloc[i, 0].item()\n",
    "    ub = opt.iloc[i, 1].item()\n",
    "\n",
    "    # Mask\n",
    "    mask = donuts[hour] & df['r'].between(lb, ub)\n",
    "\n",
    "    # Fit models\n",
    "    m = sm.OLS.from_formula(\n",
    "        formula=f'ncr1Post{hour} ~ treat + r + rTreat',\n",
    "        data=df[mask]\n",
    "    ).fit(cov_type='HC0')\n",
    "\n",
    "    # Append model to list\n",
    "    m_opt.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b82d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init list\n",
    "d = []\n",
    "\n",
    "# Extract results from each winning model\n",
    "for i in range(len(m_opt)):\n",
    "    d.append(\n",
    "        [\n",
    "            m_opt[i].params['treat'],\n",
    "            m_opt[i].bse['treat'],\n",
    "            m_opt[i].pvalues['treat'],\n",
    "            stars(\n",
    "                m_opt[i].pvalues['treat']\n",
    "            ),\n",
    "            m_opt[i].predict(\n",
    "                {'treat':0, 'r':0, 'rTreat':0}\n",
    "            ).item(),\n",
    "            m_opt[i].rsquared,\n",
    "            int(m_opt[i].nobs)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Results to dataframe\n",
    "d = pd.DataFrame(data=d, columns=['T', 'se', 'pval', 'signif', 'mean', 'R2', 'nobs'])\n",
    "\n",
    "# View results\n",
    "d.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e61a0",
   "metadata": {},
   "source": [
    "#### 4.2.3. Fake cutoffs\n",
    "Assume fake cutoffs each 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899369e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in hours:\n",
    "    df[f'rFake{h}'] = df['r'] - (h * 60**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33cb6b5",
   "metadata": {
    "id": "f33cb6b5"
   },
   "source": [
    "### 4.4. Visualizations\n",
    "Create dataframe with predictions from models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89626b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init data\n",
    "d = pd.DataFrame({'r':np.arange(-5*24*60**2, 5*24*60**2, 60**2)})\n",
    "\n",
    "# Assign treatment\n",
    "d['treat'] = np.where(d['r'].le(0), 0, 1)\n",
    "\n",
    "# Init index counter\n",
    "i = 0\n",
    "\n",
    "# Iterate over targets\n",
    "for target in ['ncr1', 'ncr2', 'pcr1', 'pcr2', 'rel1', 'rel2']:\n",
    "\n",
    "    # Iterate over Post%H windows\n",
    "    for h, post in zip(hours, windows):\n",
    "\n",
    "        # Select model\n",
    "        model = d1[i]\n",
    "\n",
    "        # Init empty list\n",
    "        preds = []\n",
    "\n",
    "        # Make predictions\n",
    "        for idx, vals in d.iterrows():\n",
    "            pred = model.predict({'r':vals['r'], 'treat':vals['treat']})\n",
    "            preds.append(pred.item())\n",
    "\n",
    "        # Add predictions to dataframe\n",
    "        d[f'{target}{post}'] = preds\n",
    "\n",
    "        # Add 1 to counter\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb961bd",
   "metadata": {},
   "source": [
    "Create plots for `ncr1` models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target name\n",
    "target = 'ncr1'\n",
    "\n",
    "# Init figure\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, sharex=True, sharey=True)\n",
    "fig.set_figheight(8)\n",
    "fig.set_figwidth(8)\n",
    "\n",
    "# Iterate over hours\n",
    "for i, h in enumerate(hours):\n",
    "\n",
    "    # Flatten axes\n",
    "    ax = axs.flatten()[i]\n",
    "\n",
    "    # Masks\n",
    "    pre = d['r'].le(-h*60**2)\n",
    "    pre_hat = d['r'].between(-h*60**2, 0)\n",
    "    post_hat = d['r'].between(60**2, 24*60**2)\n",
    "    post = d['r'].ge(24*60**2)\n",
    "\n",
    "    # Predicted lines\n",
    "    ax.plot(d.loc[pre, 'r'], d.loc[pre, f'{target}Post{h}'], color='C0')\n",
    "    ax.plot(d.loc[pre_hat, 'r'], d.loc[pre_hat, f'{target}Post{h}'], color='C1', ls='--')\n",
    "    ax.plot(d.loc[post_hat, 'r'], d.loc[post_hat, f'{target}Post{h}'], color='C1', ls='--')\n",
    "    ax.plot(d.loc[post, 'r'], d.loc[post, f'{target}Post{h}'], color='C0')\n",
    "\n",
    "    # Shaded regions\n",
    "    ax.axvspan(xmin=-h*60**2, xmax=0, color='C0', alpha=0.5)\n",
    "    ax.axvspan(xmin=0, xmax=24*60**2, color='gray', alpha=0.5)\n",
    "    \n",
    "    # Aesthetics\n",
    "    ax.set_title(f'First {h} hours', fontsize=10)\n",
    "    ax.set_xticks(np.linspace(-5*24*60**2, 5*24*60**2+1, 11))\n",
    "    ax.set_xticklabels(np.arange(-5, 5+1))\n",
    "    if i > 3:\n",
    "        ax.set_xlabel('Days since November 10')\n",
    "    if i % 2 == 0:\n",
    "        ax.set_ylabel('NCR')\n",
    "\n",
    "    # Get grouped means from targets\n",
    "    t = df[~df['r'].between(-h*60**2, 24*60**2)].copy()\n",
    "    t['bins'] = pd.qcut(t['r'], 35)\n",
    "    x = t.groupby('bins')[f'{target}Post{h}'].mean()\n",
    "    \n",
    "    # Plot bins from targets\n",
    "    ax.scatter(\n",
    "        [val.left for val in x.index.values],\n",
    "        x.values,\n",
    "        color='C4',\n",
    "        alpha=0.5,\n",
    "        s=15\n",
    "    )\n",
    "\n",
    "ax.set_ylim(0, 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81349734",
   "metadata": {},
   "source": [
    "Create plots for `ncr2` models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a6cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target name\n",
    "target = 'ncr2'\n",
    "\n",
    "# Init figure\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, sharex=True, sharey=True)\n",
    "fig.set_figheight(8)\n",
    "fig.set_figwidth(8)\n",
    "\n",
    "# Iterate over hours\n",
    "for i, h in enumerate(hours):\n",
    "\n",
    "    # Flatten axes\n",
    "    ax = axs.flatten()[i]\n",
    "\n",
    "    # Masks\n",
    "    pre = d['r'].le(-h*60**2)\n",
    "    pre_hat = d['r'].between(-h*60**2, 0)\n",
    "    post_hat = d['r'].between(60**2, 24*60**2)\n",
    "    post = d['r'].ge(24*60**2)\n",
    "\n",
    "    # Predicted lines\n",
    "    ax.plot(d.loc[pre, 'r'], d.loc[pre, f'{target}Post{h}'], color='C0')\n",
    "    ax.plot(d.loc[pre_hat, 'r'], d.loc[pre_hat, f'{target}Post{h}'], color='C1', ls='--')\n",
    "    ax.plot(d.loc[post_hat, 'r'], d.loc[post_hat, f'{target}Post{h}'], color='C1', ls='--')\n",
    "    ax.plot(d.loc[post, 'r'], d.loc[post, f'{target}Post{h}'], color='C0')\n",
    "\n",
    "    # Shaded regions\n",
    "    ax.axvspan(xmin=-h*60**2, xmax=0, color='C0', alpha=0.5)\n",
    "    ax.axvspan(xmin=0, xmax=24*60**2, color='gray', alpha=0.5)\n",
    "    \n",
    "    # Aesthetics\n",
    "    ax.set_title(f'First {h} hours', fontsize=10)\n",
    "    ax.set_xticks(np.linspace(-5*24*60**2, 5*24*60**2+1, 11))\n",
    "    ax.set_xticklabels(np.arange(-5, 5+1))\n",
    "    if i > 3:\n",
    "        ax.set_xlabel('Days since November 10')\n",
    "    if i % 2 == 0:\n",
    "        ax.set_ylabel('sNCR')\n",
    "\n",
    "    # Get grouped means from targets\n",
    "    t = df[~df['r'].between(-h*60**2, 24*60**2)].copy()\n",
    "    t['bins'] = pd.qcut(t['r'], 35)\n",
    "    x = t.groupby('bins')[f'{target}Post{h}'].mean()\n",
    "    \n",
    "    # Plot bins from targets\n",
    "    ax.scatter(\n",
    "        [val.left for val in x.index.values],\n",
    "        x.values,\n",
    "        color='C4',\n",
    "        alpha=0.5,\n",
    "        s=15\n",
    "    )\n",
    "\n",
    "ax.set_ylim(0, 0.8)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "01_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "95c0f0991ffcdb038fcf97c3f0d49464981cd58006e8af0cf678ddb2337346a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
